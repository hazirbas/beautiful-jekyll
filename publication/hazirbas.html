
<table>

<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="hazirbas17ddff">1</a>]
</td>
<td class="bibtexitem">
C.&nbsp;Hazirbas, L.&nbsp;Leal-Taixé, and D.&nbsp;Cremers, &ldquo;<b>Deep Depth From
  Focus</b>,&rdquo; in <em>Arxiv</em>, April 2017.<br />
[&nbsp;<a href="hazirbas_bib.html#hazirbas17ddff">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1704.01085">arXiv</a>&nbsp;]
<blockquote><font size="-1">
Depth from Focus (DFF) is one of the classical ill-posed inverse problems in computer vision. Most approaches recover the depth at each pixel based on the focal setting which exhibits maximal sharpness. Yet, it is not obvious how to reliably estimate the sharpness level, particularly in low-textured areas.  In this paper, we propose `Deep Depth From Focus (DDFF)' as the first end-to-end learning approach to this problem. Towards this goal, we create a novel real-scene indoor benchmark composed of 4D light-field images obtained from a plenoptic camera and  ground truth depth obtained from a registered RGB-D sensor. Compared to existing benchmarks our dataset is 30 times larger, enabling the use of machine learning for this inverse problem. We compare our results with state-of-the-art DFF methods and we also analyze the effect of several key deep architectural components.  These experiments show that DDFFNet achieves state-of-the-art performance in all scenes, reducing depth error by more than 70% wrt classic DFF methods.
</font></blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="walch16spatialstms">2</a>]
</td>
<td class="bibtexitem">
F.&nbsp;Walch, C.&nbsp;Hazirbas, L.&nbsp;Leal-Taixé, T.&nbsp;Sattler, S.&nbsp;Hilsenbeck, and
  D.&nbsp;Cremers, &ldquo;<b>Image-based localization using LSTMs for structured
  feature correlation</b>,&rdquo; in <em>Arxiv</em>, March 2017.<br />
[&nbsp;<a href="hazirbas_bib.html#walch16spatialstms">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1611.07890">arXiv</a>&nbsp;]
<blockquote><font size="-1">
In this work we propose a new CNN+LSTM architecture for camera pose regression for indoor and outdoor scenes. CNNs allow us to learn suitable feature representations for localization that are robust against motion blur and illumination changes. We make use of LSTM units on the CNN output, which play the role of a structured dimensionality reduction on the feature vector, leading to drastic improvements in localization performance. We provide extensive quantitative comparison of CNN-based vs SIFT-based localization methods, showing the weaknesses and strengths of each. Furthermore, we present a new large-scale indoor sequence with accurate ground truth from a laser scanner. Experimental results on both indoor and outdoor public datasets show our method outperforms existing deep architectures, and can localize images in hard conditions, e.g., in the presence of mostly textureless surfaces, where classic SIFT-based methods fail.
</font></blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="hazirbas16fusenet">3</a>]
</td>
<td class="bibtexitem">
C.&nbsp;Hazirbas, L.&nbsp;Ma, C.&nbsp;Domokos, and D.&nbsp;Cremers, &ldquo;<b>FuseNet:
  Incorporating Depth into Semantic Segmentation via Fusion-based CNN
  Architecture</b>,&rdquo; in <em>Asian Conference on Computer Vision (ACCV)</em>,
  November 2016.<br />
[&nbsp;<a href="hazirbas_bib.html#hazirbas16fusenet">bib</a>&nbsp;| 
<a href="https://github.com/tum-vision/fusenet">source</a>&nbsp;]
<blockquote><font size="-1">
In this paper we address the problem of semantic labeling of indoor scenes on RGB-D data. With the availability of RGB-D cameras, it is expected that additional depth measurement will improve the accuracy. Here we investigate a solution how to incorporate complementary depth information into a semantic segmentation framework by making use of convolutional neural networks (CNNs). Recently encoder-decoder type fully convolutional CNN architectures have achieved a great success in the field of semantic segmentation. Motivated by this observation we propose an encoder-decoder type network, where the encoder part is composed of two branches of networks that simultaneously extract features from RGB and depth images and fuse depth features into the RGB feature maps as the network goes deeper. Comprehensive experimental evaluations demonstrate that the proposed fusion-based architecture achieves competitive results with the state-of-the-art methods on the challenging SUN RGB-D benchmark obtaining 76.27% global accuracy, 48.30% average class accuracy and 37.29% average intersection-over-union score.
</font></blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="dosovitskiy15flownet">4</a>]
</td>
<td class="bibtexitem">
A.&nbsp;Dosovitskiy, P.&nbsp;Fischer, E.&nbsp;Ilg, P.&nbsp;Haeusser, C.&nbsp;Hazirbas, V.&nbsp;Golkov,
  P.&nbsp;van&nbsp;der Smagt, D.&nbsp;Cremers, and T.&nbsp;Brox, &ldquo;<b>FlowNet: Learning
  Optical Flow with Convolutional Networks</b>,&rdquo; in <em>IEEE International
  Conference on Computer Vision (ICCV)</em>, December 2015.<br />
[&nbsp;<a href="hazirbas_bib.html#dosovitskiy15flownet">bib</a>&nbsp;| 
<a href="10.1109/ICCV.2015.316">doi</a>&nbsp;]
<blockquote><font size="-1">
Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks where CNNs were successful. In this paper we construct appropriate CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations. <br>
 Since existing ground truth data sets are not sufficiently large to train a CNN, we generate a synthetic Flying Chairs dataset. We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps.
</font></blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="stark15captcha">5</a>]
</td>
<td class="bibtexitem">
F.&nbsp;Stark, C.&nbsp;Hazirbas, R.&nbsp;Triebel, and D.&nbsp;Cremers, &ldquo;<b>CAPTCHA
  Recognition with Active Deep Learning</b>,&rdquo; in <em>GCPR Workshop on New
  Challenges in Neural Computation</em>, October 2015.<br />
[&nbsp;<a href="hazirbas_bib.html#stark15captcha">bib</a>&nbsp;| 
<a href="https://github.com/tum-vision/captcha_recognition">source</a>&nbsp;]
<blockquote><font size="-1">
<tt>CAPTCHA</tt>s are automated tests to tell computers and humans apart. They are designed to be easily solvable by humans, but unsolvable by machines. With Convolutional Neural Networks these tests can also be solved automatically. However, the strength of CNNs relies on the training data that the classifier is learnt on and especially on the size of the training set. Hence, it is intractable to solve the problem with CNNs in case of insufficient training data. We propose an Active Deep Learning strategy that makes use of the ability to gain new training data for free without any human intervention which is possible in the special case of CAPTCHAs. We discuss how to choose the new samples to re-train the network and present results on an auto-generated CAPTCHA dataset. Our approach dramatically improves the performance of the network if we initially have only few labeled training data.
</font></blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="hazirbas15afs">6</a>]
</td>
<td class="bibtexitem">
C.&nbsp;Hazirbas, J.&nbsp;Diebold, and D.&nbsp;Cremers, &ldquo;<b>Optimizing the
  Relevance-Redundancy Tradeoff for Efficient Semantic Segmentation</b>,&rdquo; in <em>
  Scale Space and Variational Methods in Computer Vision (SSVM)</em>, June 2015.
 <b>Oral Presentation</b>.<br />
[&nbsp;<a href="hazirbas_bib.html#hazirbas15afs">bib</a>&nbsp;| 
<a href="10.1007/978-3-319-18461-6_20">doi</a>&nbsp;| 
<a href="https://github.com/tum-vision/AFS">source</a>&nbsp;]
<blockquote><font size="-1">
Semantic segmentation aims at jointly computing a segmentation and a semantic labeling of the image plane. The main ingredient is an efficient feature selection strategy. In this work we perform a systematic information-theoretic evaluation of existing features in order to address the question which and how many features are appropriate for an efficient semantic segmentation. To this end, we discuss the tradeoff between relevance and redundancy and present an informationtheoretic feature evaluation strategy. Subsequently, we perform a systematic experimental validation which shows that the proposed feature selection strategy provides state-of-the-art semantic segmentations on five semantic segmentation datasets at significantly reduced runtimes. Moreover, it provides a systematic overview of which features are the most relevant for various benchmarks.
</font></blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="diebold15scribble">7</a>]
</td>
<td class="bibtexitem">
J.&nbsp;Diebold, N.&nbsp;Demmel, C.&nbsp;Hazirbas, M.&nbsp;Möller, and D.&nbsp;Cremers,
  &ldquo;<b>Interactive Multi-label Segmentation of RGB-D Images</b>,&rdquo; in <em>
  Scale Space and Variational Methods in Computer Vision (SSVM)</em>, June 2015.<br />
[&nbsp;<a href="hazirbas_bib.html#diebold15scribble">bib</a>&nbsp;| 
<a href="10.1007/978-3-319-18461-6_24">doi</a>&nbsp;| 
<a href="https://github.com/NikolausDemmel/tvseg">source</a>&nbsp;]
<blockquote><font size="-1">
We propose a novel interactive multi-label RGB-D image segmentation method by extending spatially varying color distributions [14] to additionally utilize depth information in two different ways. On the one hand, we consider the depth image as an additional data channel. On the other hand, we extend the idea of spatially varying color distributions in a plane to volumetrically varying color distributions in 3D. Furthermore, we improve the data fidelity term by locally adapting the influence of nearby scribbles around each pixel. Our approach is implemented for parallel hardware and evaluated on a novel interactive RGB-D image segmentation benchmark with pixel-accurate ground truth. We show that depth information leads to considerably more precise segmentation results. At the same time significantly less user scribbles are required for obtaining the same segmentation accuracy as without using depth clues.
</font></blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="hazirbas14msc">8</a>]
</td>
<td class="bibtexitem">
C.&nbsp;Hazirbas, &ldquo;<b>Feature Selection and Learning for Semantic
  Segmentation</b>,&rdquo; Master's thesis, Technical University Munich, Munich,
  Germany, 2014.<br />
[&nbsp;<a href="hazirbas_bib.html#hazirbas14msc">bib</a>&nbsp;]
<blockquote><font size="-1">
This work presents a comprehensive study on feature selection and learning for semantic segmentation. Various types of features, different learning algorithms in conjunction with minimizing a variational formulation, are discussed in order to obtain the best segmentation of the scene with minimal redundancy in the feature set. The features are scored in terms of relevance and redundancy. A clever feature selection reduces not only the redundancy but also the computational cost of object detection. Additionally different learning algorithms are studied and the most suitable multi-class object classifier is trained with the selected subset of features for detection based unary potential computation. In order to obtain consistent segmentation results we minimize a variational formulation of the multi-labelling problem by means of first order primal-dual optimization.<br>
  Experiments on different benchmarks give a deep understanding on how many and what kind of features and which learning algorithm should be used for semantic segmentation.
</font></blockquote>

</td>
</tr>
</table>